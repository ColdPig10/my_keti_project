import os
from dotenv import load_dotenv
load_dotenv()  # .envì—ì„œ OPENAI_API_KEY ì½ì–´ í™˜ê²½ ë³€ìˆ˜ì— ì„¸íŒ…

#langchain_communicty
from langchain_community.chat_models import ChatOpenAI
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.embeddings.openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains import RetrievalQAWithSourcesChain

#Streamlit ì—°ë™
import sqlite3
import sys
import streamlit as st

#ì±—ë´‡ ì„¤ì •
st.set_page_config(
    page_title="ì§ˆì˜ì‘ë‹µì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# OPENAI API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

# PDF ë¬¸ì„œ ë¡œë“œ
loader = PyPDFLoader('parkinglot_law.pdf')  # íŒŒì¼ í™•ì¥ì í™•ì¸
documents = loader.load()

# ë¬¸ì„œ ë¶„í• 
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

# ì„ë² ë”© ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
vector_store = Chroma.from_documents(texts, embeddings)

# ê²€ìƒ‰ê¸° ìƒì„±
retriever = vector_store.as_retriever(search_kwargs={"k": 2})

#í”„ë¡¬í”„íŠ¸ ê°œì„ 
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

system_template="""
Use the following pieces of context to answer the user's question.
If you don't know the answer, just say "I don't know", don't try to make up an answer.
----------------
{context}

You MUST answer in Korean and in Markdown format. Also include source information as "SOURCES".
"""

messages = [
    SystemMessagePromptTemplate.from_template(system_template),
    HumanMessagePromptTemplate.from_template("{question}")
]

prompt = ChatPromptTemplate.from_messages(messages)

#ChatGPT ëª¨ë¸ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ
chain_type_kwargs = {
    "prompt": prompt,
    "document_variable_name": "context"  # ì´ ì¤„ ì¶”ê°€!
}

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)  # Modify model_name if you have access to GPT-4

chain = RetrievalQAWithSourcesChain.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever = retriever,
    return_source_documents=True,
    chain_type_kwargs=chain_type_kwargs
)

#streamlit ì „ìš© ì±—ë´‡ ì½”ë“œ
st.subheader('ì§ˆë¬¸ì„ ì ì–´ ì£¼ì„¸ìš”')

#ì‘ë‹µ ìƒì„± ì½”ë“œ
def generate_response(input_text):
    result = chain({"question": input_text})
    return result["answer"]

if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ì§ˆë¬¸ì„ ì ì–´ ì£¼ì„¸ìš” ë¬´ì—‡ì„ ë„ì™€ ë“œë¦´ê¹Œìš”?"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    msg =  generate_response(prompt)
    st.session_state.messages.append({"role": "assistant", "content": msg})
    st.chat_message("assistant").write(msg)